{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Emotion Detection Challenge on Twitter**"
      ],
      "metadata": {
        "id": "4sNid1i7uRdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine sifting through the endless stream of tweets and figuring out the prevailing emotion. In this challenge, you won't just focus on classic positive or negative sentiment. Instead, you'll tackle the more intricate task of identifying four core emotions:\n",
        "\n",
        "\n",
        "1.   ðŸ˜  **Anger**     (class 0)\n",
        "2.   ðŸ˜‚ **Joy**       (class 1)\n",
        "3.   ðŸ˜€ **Optimism** (class 2)\n",
        "4.   ðŸ˜ž **Sadness**   (class 3)\n",
        "\n",
        "Your goal? Assign the most dominant emotion to each tweet. Sounds fun, right? Let's see how you handle the nuances of human feelings, all packed into 280 characters!"
      ],
      "metadata": {
        "id": "8qzTv251ucVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Loading the data**"
      ],
      "metadata": {
        "id": "bTj40z06wV6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Library"
      ],
      "metadata": {
        "id": "q2ljwmwh7qsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug\n",
        "!pip install contractions\n",
        "!pip install wordsegment\n",
        "!pip install emoji"
      ],
      "metadata": {
        "id": "fTXzHrj0hvHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.tag import pos_tag\n",
        "from string import punctuation\n",
        "import re\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.optim as optim\n",
        "\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "ixRqRL5p89yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "N-JtOdgBza78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jWqYoK5CauGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "4piM8S0N7umj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozcpfSDYuLpu"
      },
      "outputs": [],
      "source": [
        "file_path_train = '/content/drive/MyDrive/Emotional_Detection/train_text.txt'\n",
        "file_path_valid = '/content/drive/MyDrive/Emotional_Detection/val_text.txt'\n",
        "file_path_test = '/content/drive/MyDrive/Emotional_Detection/test_text.txt'\n",
        "\n",
        "## Train\n",
        "with open(file_path_train, 'r') as file:\n",
        "    righe = file.readlines()\n",
        "\n",
        "righe = [riga.strip() for riga in righe]\n",
        "\n",
        "df_train = pd.DataFrame(righe, columns=['text'])\n",
        "\n",
        "## Valid\n",
        "with open(file_path_valid, 'r') as file:\n",
        "    righe = file.readlines()\n",
        "\n",
        "righe = [riga.strip() for riga in righe]\n",
        "\n",
        "df_valid = pd.DataFrame(righe, columns=['text'])\n",
        "\n",
        "## Test\n",
        "\n",
        "with open(file_path_test, 'r') as file:\n",
        "    righe = file.readlines()\n",
        "\n",
        "righe = [riga.strip() for riga in righe]\n",
        "\n",
        "df_test = pd.DataFrame(righe, columns=['text'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = pd.read_csv(\"/content/drive/MyDrive/Emotional_Detection/train_labels.txt\", header=None)\n",
        "Y_valid = pd.read_csv(\"/content/drive/MyDrive/Emotional_Detection/val_labels.txt\", header=None)\n",
        "\n",
        "with open(file_path_train, 'r') as file:\n",
        "    righe = file.readlines()\n",
        "\n",
        "righe = [riga.strip() for riga in righe]\n",
        "\n",
        "X_for_graph = pd.DataFrame(righe, columns=['text'])\n"
      ],
      "metadata": {
        "id": "CmU8VHfyF2A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizzation"
      ],
      "metadata": {
        "id": "GYIkRnYS71Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.tail()"
      ],
      "metadata": {
        "id": "_sqivCTJAIJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.tail()"
      ],
      "metadata": {
        "id": "ls0h9EJdtVHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "UyRG3v2HtY-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "id": "uwc0TSiHteMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nChecking for missing values\")\n",
        "df_train.isnull().sum()"
      ],
      "metadata": {
        "id": "EXd7h9XHtjbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Count of sentiment wise values: \\n\", Y_train.value_counts())"
      ],
      "metadata": {
        "id": "Jzf-wWyktqTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_train)"
      ],
      "metadata": {
        "id": "raydQBdvzLMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Data Analysis & Processing**"
      ],
      "metadata": {
        "id": "6di-r44g7vEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Analysis**"
      ],
      "metadata": {
        "id": "iWCliU5H62Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig=px.histogram(Y_train,\n",
        "                title=\"Sentiment Count \",\n",
        "                color_discrete_sequence=[\"red\"])\n",
        "fig.update_layout(bargap=0.1)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "XQWokNd-wc7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dall'istogramma notiamo che le classi sono sbilanciate, con la classe 2 poco presente e la classe 0 molto presente"
      ],
      "metadata": {
        "id": "vVyR0L1o1Ha3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_length(tweet):\n",
        "    str_len=len(tweet.split(\" \"))\n",
        "    return(str_len)\n",
        "\n",
        "X_for_graph['Length'] = X_for_graph['text'].apply(lambda x:text_length(x))\n",
        "\n",
        "\n",
        "fig = px.histogram(X_for_graph,\n",
        "                  x='Length',\n",
        "                  marginal='box',\n",
        "                  title=\"Length of tweets text\")\n",
        "fig.update_layout(bargap=0.1)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "UlDAqXh61EAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dall'istogramma che mostra il numero di parole per i vari testi notiamo che Ã¨ presente un outliers con una lunghezza di 58 parole, mentre il restante si aggira tra 3 e 33 parole"
      ],
      "metadata": {
        "id": "trcqzq2Q2Kvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_length(tweet):\n",
        "    str_len = len(tweet.split(\" \"))\n",
        "    return str_len\n",
        "\n",
        "data_train = X_for_graph\n",
        "data_train['Length'] = data_train['text'].apply(lambda x: text_length(x))\n",
        "\n",
        "# Combina il dataset con le etichette\n",
        "dataframe_train = pd.concat([data_train, Y_train], axis=1)\n",
        "dataframe_train.columns = ['Text', 'Length', 'Label']\n",
        "\n",
        "unique_labels = dataframe_train['Label'].unique()\n",
        "\n",
        "colors = px.colors.qualitative.Set1\n",
        "\n",
        "# Crea un istogramma separato per ogni classe con un colore diverso\n",
        "for i, label in enumerate(unique_labels):\n",
        "    df_class = dataframe_train[dataframe_train['Label'] == label]\n",
        "\n",
        "    # Crea l'istogramma per la classe\n",
        "    fig = px.histogram(df_class,\n",
        "                       x='Length',\n",
        "                       marginal='box',\n",
        "                       title=f\"Length of tweets text for class {label}\",\n",
        "                       color_discrete_sequence=[colors[i % len(colors)]]\n",
        "                      )\n",
        "    fig.update_layout(bargap=0.1)\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "twDRTcYq3PCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rappresentiamo gli istogrammi del numero di parole nel testo per ogni classe, cercando di identificare se il sentimento che si prova impatta nella lunghezza del testo. Sembrerebbe che le persone nervose tendino a scrivere testi piÃ¹ lunghi. Le persone felici testi un pochino piÃ¹ corti."
      ],
      "metadata": {
        "id": "QPv_bM4Y4yW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Processing**"
      ],
      "metadata": {
        "id": "q6JdwvQm66x4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove stop words, digits, and punctuation and lowercase a given collection of texts"
      ],
      "metadata": {
        "id": "QYvdcykX9w72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def count_repeated_hashtags(text_lines):\n",
        "    \"\"\"\n",
        "    Funzione che estrae e conta gli hashtag ripetuti almeno 5 volte da una lista di righe di testo.\n",
        "\n",
        "    Args:\n",
        "    text_lines (list of str): Lista contenente le righe di testo.\n",
        "\n",
        "    Returns:\n",
        "    dict: Dizionario con gli hashtag ripetuti almeno 5 volte e il loro conteggio.\n",
        "    \"\"\"\n",
        "    hashtag_counter = defaultdict(int)  # Dizionario per contare le occorrenze degli hashtag\n",
        "\n",
        "    # Itera attraverso le righe del testo\n",
        "    for line in text_lines:\n",
        "        # Trova tutti gli hashtag in ogni riga\n",
        "        hashtags = re.findall(r'#\\w+', line)\n",
        "        # Aggiorna il conteggio degli hashtag trovati\n",
        "        for hashtag in hashtags:\n",
        "            hashtag_counter[hashtag] += 1\n",
        "\n",
        "    # Filtra gli hashtag che si ripetono almeno 5 volte\n",
        "    frequent_hashtags = {hashtag: count for hashtag, count in hashtag_counter.items() if count >= 5}\n",
        "\n",
        "    return frequent_hashtags\n",
        "\n",
        "\n",
        "# Chiamata alla funzione\n",
        "repeated_hashtags = count_repeated_hashtags(df_train[\"text\"])\n",
        "\n",
        "# Stampa gli hashtag che si ripetono almeno 5 volte\n",
        "print(repeated_hashtags)\n"
      ],
      "metadata": {
        "id": "VkFADTbGvcj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def handle_negations(text):\n",
        "    tokens = text.split()\n",
        "    negations = {\"not\", \"no\", \"never\", \"n't\"}\n",
        "    result = []\n",
        "    negate = False\n",
        "    for token in tokens:\n",
        "        lower_token = token.lower()\n",
        "        if negate and token not in string.punctuation:\n",
        "            result.append(token + \"_NEG\")\n",
        "        else:\n",
        "            result.append(token)\n",
        "        negate = lower_token in negations\n",
        "    return ' '.join(result)"
      ],
      "metadata": {
        "id": "gVaefIXAwdoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "import html\n",
        "from nltk.corpus import stopwords\n",
        "from contractions import fix\n",
        "import wordsegment\n",
        "wordsegment.load()\n",
        "import emoji\n",
        "import unicodedata\n",
        "\n",
        "# Initialize spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define stopwords\n",
        "mystopwords = set(stopwords.words(\"english\"))\n",
        "negation_words = {\"not\", \"no\", \"nor\", \"never\", \"n't\"}\n",
        "additional_words_to_keep = {\"but\", \"against\", \"without\", \"won\", \"don't\", \"can't\", \"couldn't\"}\n",
        "words_to_keep = negation_words.union(additional_words_to_keep)\n",
        "\n",
        "# Remove these words from the stopword list\n",
        "mystopwords = mystopwords - words_to_keep\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Removes URLs, processes mentions and hashtags, handles HTML entities,\n",
        "    expands contractions, replaces slang, handles emojis, and converts everything to lowercase.\n",
        "    \"\"\"\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Replace HTML entities with their corresponding characters\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    # Remove punctuation but keep repeated question marks and exclamation points\n",
        "    text = re.sub(r'(?<!\\?)\\?(?!\\?)|(?<!\\!)\\!(?!\\!)', '', text)\n",
        "\n",
        "\n",
        "    # Normalize unicode characters\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "\n",
        "    # Expand contractions\n",
        "    text = fix(text)\n",
        "\n",
        "    # Handle negations by appending _NEG to the following word\n",
        "    text = handle_negations(text)\n",
        "\n",
        "    important_hashtags = repeated_hashtags\n",
        "    text = re.sub(r'#(\\w+)', lambda m: m.group(0) if m.group(0) in important_hashtags else ' '.join(wordsegment.segment(m.group(0)[1:])), text)\n",
        "\n",
        "    # Replace mentions with placeholder\n",
        "    text = re.sub(r'@\\w+', '<USER>', text)\n",
        "\n",
        "    # Replace slang terms\n",
        "    slang_dict = {\n",
        "        \"u\": \"you\",\n",
        "        \"bcuz\": \"because\",\n",
        "        \"gonna\": \"going to\",\n",
        "        \"prolly\": \"probably\",\n",
        "        \"tho\": \"though\",\n",
        "        \"tbh\": \"to be honest\",\n",
        "        \"idk\": \"I do not know\",\n",
        "        \"im\": \"I am\",\n",
        "        \"cant\": \"cannot\",\n",
        "        \"wanna\": \"want to\",\n",
        "        \"gimme\": \"give me\",\n",
        "        \"gotta\": \"got to\",\n",
        "        \"kinda\": \"kind of\",\n",
        "        \"luv\": \"love\",\n",
        "        \"yall\": \"you all\",\n",
        "        \"ya\": \"you\",\n",
        "        \"dunno\": \"do not know\",\n",
        "        \"btw\": \"by the way\",\n",
        "        \"thx\": \"thanks\",\n",
        "        \"omg\": \"oh my god\",\n",
        "\n",
        "    }\n",
        "    def replace_slang(text):\n",
        "        tokens = text.split()\n",
        "        tokens = [slang_dict.get(token.lower(), token) for token in tokens]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    text = replace_slang(text)\n",
        "\n",
        "    # Handle emojis: convert emojis to text\n",
        "    text = emoji.demojize(text, delimiters=(' ', ' '))\n",
        "\n",
        "    # Remove punctuation (except for emoji descriptions)\n",
        "    text = re.sub(r'[^\\w\\s_]', '', text)\n",
        "\n",
        "    # Remove extra whitespace and newlines\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.lower()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Performs text preprocessing with normalization, stopword removal,\n",
        "    and lemmatization.\n",
        "    \"\"\"\n",
        "    text = normalize_text(text)\n",
        "\n",
        "    # Process text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Tokenization and lemmatization\n",
        "    tokens = [\n",
        "        token.lemma_ for token in doc\n",
        "        if token.text.lower() not in mystopwords and not token.is_punct and not token.like_num\n",
        "    ]\n",
        "\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "4MbiDwbV9HC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data agumentation"
      ],
      "metadata": {
        "id": "fykkUDGLkjer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synonym_aug = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "def augment_dataset(df, label_column, text_column, augmenter, n=1):\n",
        "    augmented_texts = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        text = row[text_column]\n",
        "        label = row[label_column]\n",
        "\n",
        "        augmented_versions = augment_text(text, augmenter, n)\n",
        "        augmented_texts.extend(augmented_versions)\n",
        "        augmented_labels.extend([label] * len(augmented_versions))\n",
        "\n",
        "    augmented_df = pd.DataFrame({\n",
        "        text_column: augmented_texts,\n",
        "        label_column: augmented_labels\n",
        "    })\n",
        "\n",
        "    return augmented_df\n",
        "\n",
        "# Applica l'augmentation al dataset\n",
        "def augment_text(text, augmenter, n=1):\n",
        "    # Genera n versioni aumentate del testo\n",
        "    augmented_texts = augmenter.augment(text, n=n)\n",
        "\n",
        "    # Assicurati che augmented_texts sia una lista di stringhe\n",
        "    if isinstance(augmented_texts, str):\n",
        "        augmented_texts = [augmented_texts]\n",
        "    elif isinstance(augmented_texts, list):\n",
        "        # Verifica che tutti gli elementi siano stringhe\n",
        "        augmented_texts = [str(t) for t in augmented_texts]\n",
        "    else:\n",
        "        # In caso di tipi inattesi, convertili in stringhe\n",
        "        augmented_texts = [str(t) for t in augmented_texts]\n",
        "\n",
        "    return augmented_texts\n",
        "\n",
        "\n",
        "df_train = pd.concat([df_train, Y_train], axis=1)\n",
        "df_train.columns = ['processed_text', \"label\"]\n",
        "\n",
        "# Applica l'augmentation solo alle classi minoritarie\n",
        "class_counts = df_train['label'].value_counts()\n",
        "max_count = class_counts.max()\n",
        "minority_classes = class_counts[class_counts < max_count].index.tolist()  # Esclude la classe maggioritaria\n",
        "df_minority = df_train[df_train['label'].isin(minority_classes)]\n",
        "\n",
        "# Applica l'augmentation\n",
        "augmented_df = augment_dataset(df_minority, 'label', 'processed_text', synonym_aug, n=1)\n",
        "\n",
        "# Unisci il dataset originale con quello aumentato\n",
        "df_train_augmented = pd.concat([df_train, augmented_df]).reset_index(drop=True)\n",
        "print(df_train_augmented['label'].value_counts())"
      ],
      "metadata": {
        "id": "h3yPxKTokWo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Applichiamo il pre processing"
      ],
      "metadata": {
        "id": "h4J6U83hh3jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_corpus(texts):\n",
        "    return [preprocess_text(text) for text in texts]\n",
        "\n",
        "\n",
        "df_train_augmented['processed_text'] = df_train_augmented['processed_text'].apply(preprocess_text)\n",
        "#df_train.drop('text', axis=1, inplace=True)\n",
        "\n",
        "df_valid['processed_text'] = df_valid['text'].apply(preprocess_text)\n",
        "df_valid.drop('text', axis=1, inplace=True)\n",
        "\n",
        "df_test['processed_text'] = df_test['text'].apply(preprocess_text)\n",
        "df_test.drop('text', axis=1, inplace=True)\n",
        "\n",
        "#df_train = pd.concat([df_train, Y_train], axis=1)\n",
        "#df_train.columns = ['processed_text', 'label']"
      ],
      "metadata": {
        "id": "mFWUH0i5CYlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.tail()"
      ],
      "metadata": {
        "id": "-WxfBtR8FpxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Model**"
      ],
      "metadata": {
        "id": "h5CBsnRw4hFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid['label'] = Y_valid\n",
        "df_combined = pd.concat([df_train, df_valid], axis=0).reset_index(drop=True)\n",
        "X = df_combined['processed_text']\n",
        "y = df_combined['label'].astype(int)"
      ],
      "metadata": {
        "id": "UnoAsO50UVci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_combined['label'].value_counts())"
      ],
      "metadata": {
        "id": "y499JemW7wCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vettorizzazione"
      ],
      "metadata": {
        "id": "60hlbdkRRXdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TFIDF"
      ],
      "metadata": {
        "id": "hkN0oqB2lHDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = df_test['processed_text']\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_vect = vectorizer.fit_transform(X)\n",
        "X_test_vect = vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "wcSQAzL7lAvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "E02NaXCslLrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "X_test = df_test['processed_text']\n",
        "\n",
        "# Assuming X and X_test are pandas Series containing your processed text\n",
        "X_tokenized = X.apply(lambda x: simple_preprocess(x))\n",
        "X_test_tokenized = X_test.apply(lambda x: simple_preprocess(x))\n",
        "\n",
        "# Combine all tokenized documents for training\n",
        "sentences = X_tokenized.tolist()\n",
        "\n",
        "# Train the model\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=100,    # Dimensionality of the word vectors\n",
        "    window=5,           # Maximum distance between the current and predicted word\n",
        "    min_count=1,        # Ignores all words with total frequency lower than this\n",
        "    workers=4,          # Number of worker threads to train the model\n",
        "    sg=1                # Use skip-gram; set to 0 for CBOW\n",
        ")\n",
        "\n",
        "def document_vector(doc):\n",
        "    # Filter out words that are not in the vocabulary\n",
        "    doc = [word for word in doc if word in w2v_model.wv.key_to_index]\n",
        "    # If the document is empty after filtering, return a zero vector\n",
        "    if not doc:\n",
        "        return np.zeros(w2v_model.vector_size)\n",
        "    # Compute the mean of word vectors\n",
        "    return np.mean(w2v_model.wv[doc], axis=0)\n",
        "\n",
        "X_train_vect = X_tokenized.apply(document_vector)\n",
        "X_test_vect = X_test_tokenized.apply(document_vector)\n",
        "\n",
        "X_train_vect = np.stack(X_train_vect.values)\n",
        "X_test_vect = np.stack(X_test_vect.values)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LYrOMnDcWmcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre trained embedding"
      ],
      "metadata": {
        "id": "tO_wBcj9rN0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "X_test = df_test['processed_text']\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "def get_document_embedding(text):\n",
        "    # Tokenizzazione\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    # Ottieni le ultime hidden states dal modello\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Prendi l'embedding del token [CLS] (il primo token)\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "    return embeddings.squeeze()\n",
        "# Applica la funzione ai dati di addestramento\n",
        "X_train_vect = X.apply(get_document_embedding)\n",
        "\n",
        "# Applica la funzione ai dati di test\n",
        "X_test_vect = X_test.apply(get_document_embedding)\n",
        "X_train_vect = np.vstack(X_train_vect.values)\n",
        "X_test_vect = np.vstack(X_test_vect.values)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kiquGS29muRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic regression"
      ],
      "metadata": {
        "id": "RhcCHwl_XcOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(max_iter=1000, class_weight=\"balanced\", )\n",
        "model.fit(X_train_vect, y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_vect)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "df_predictions = pd.DataFrame(y_pred, columns=['label'])\n",
        "df_predictions.to_csv('previsioni_migliori_LR.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'test_predictions.csv'\")"
      ],
      "metadata": {
        "id": "CeAm9BGI4flo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "63c6QxVqyE8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Allena il modello Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_vect, y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = nb_model.predict(X_test_vect)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "df_predictions = pd.DataFrame(y_pred, columns=['label'])\n",
        "df_predictions.to_csv('previsioni_migliori_NB.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'test_predictions.csv'\")"
      ],
      "metadata": {
        "id": "FMKpakUp1i4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGB"
      ],
      "metadata": {
        "id": "9kYd3Mu8RZYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Allena il modello XGBoost\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', class_weight=\"balanced\")\n",
        "xgb_model.fit(X_train_vect, y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test_vect)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "df_predictions = pd.DataFrame(y_pred, columns=['label'])\n",
        "df_predictions.to_csv('previsioni_migliori_XGB.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'test_predictions.csv'\")\n"
      ],
      "metadata": {
        "id": "ZQ0zkZOnRRTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "sREPWOOlmEBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Allena il modello Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
        "rf_model.fit(X_train_vect, y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test_vect)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "df_predictions = pd.DataFrame(y_pred, columns=['label'])\n",
        "df_predictions.to_csv('previsioni_migliori_RF.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'test_predictions.csv'\")\n"
      ],
      "metadata": {
        "id": "Cj4x550RdmD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Machine"
      ],
      "metadata": {
        "id": "JjQe8ctymF-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Allena il modello SVM\n",
        "svm_model = SVC(kernel='linear', probability=True, class_weight=\"balanced\")\n",
        "svm_model.fit(X_train_vect, y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test_vect)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "df_predictions = pd.DataFrame(y_pred, columns=['label'])\n",
        "df_predictions.to_csv('previsioni_migliori_SVM.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'test_predictions.csv'\")"
      ],
      "metadata": {
        "id": "Tv-_zp3fgn0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi layer perceptron"
      ],
      "metadata": {
        "id": "Z475nHXRms7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Allena il modello MLP\n",
        "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
        "mlp_model.fit(X_train_vect, y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = mlp_model.predict(X_test_vect)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "df_predictions = pd.DataFrame(y_pred, columns=['label'])\n",
        "df_predictions.to_csv('previsioni_migliori_MLP.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'test_predictions.csv'\")"
      ],
      "metadata": {
        "id": "96Iws0_TmMmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightGBM"
      ],
      "metadata": {
        "id": "IdI1JE8dyMJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "lgb_model = lgb.LGBMClassifier(class_weight=\"balanced\")\n",
        "lgb_model.fit(X_train_vect, y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = lgb_model.predict(X_test_vect)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "df_predictions = pd.DataFrame(y_pred, columns=['label'])\n",
        "df_predictions.to_csv('previsioni_migliori_LGB.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'test_predictions.csv'\")"
      ],
      "metadata": {
        "id": "RT8HDGVM2RZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADABoost"
      ],
      "metadata": {
        "id": "ctb9dJQayO-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "model_ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "model_ada.fit(X_train_vect, y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model_ada.predict(X_test_vect)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "df_predictions = pd.DataFrame(y_pred, columns=['label'])\n",
        "df_predictions.to_csv('previsioni_migliori_ADA.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'test_predictions.csv'\")"
      ],
      "metadata": {
        "id": "mCQfRY_X2aIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passive Aggressive"
      ],
      "metadata": {
        "id": "3cw1fhhdyRjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "model_PA = PassiveAggressiveClassifier(max_iter=1000, random_state=42,class_weight=\"balanced\")\n",
        "model_PA.fit(X_train_vect, y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model_PA.predict(X_test_vect)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "df_predictions = pd.DataFrame(y_pred, columns=['label'])\n",
        "df_predictions.to_csv('previsioni_migliori_PA.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'test_predictions.csv'\")"
      ],
      "metadata": {
        "id": "DBbwbfF52myn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra Tree Classify"
      ],
      "metadata": {
        "id": "oFSLIZvAyThL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "model_ETC = ExtraTreesClassifier(n_estimators=100, random_state=42,class_weight=\"balanced\")\n",
        "model_ETC.fit(X_train_vect, y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model_ETC.predict(X_test_vect)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "df_predictions = pd.DataFrame(y_pred, columns=['label'])\n",
        "df_predictions.to_csv('previsioni_migliori_ETC.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'test_predictions.csv'\")"
      ],
      "metadata": {
        "id": "cQuY_Civ2wKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting"
      ],
      "metadata": {
        "id": "hJ5YdRaAQ5jm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge prediction in a single dataframe"
      ],
      "metadata": {
        "id": "XRjhOxDnm8Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = pd.read_csv(\"/content/previsioni_migliori_LR.csv\")\n",
        "nb = pd.read_csv(\"/content/previsioni_migliori_NB.csv\")\n",
        "xgb = pd.read_csv(\"/content/previsioni_migliori_XGB.csv\")\n",
        "rf = pd.read_csv(\"/content/previsioni_migliori_RF.csv\")\n",
        "svm = pd.read_csv(\"/content/previsioni_migliori_SVM.csv\")\n",
        "mlp = pd.read_csv(\"/content/previsioni_migliori_MLP.csv\")\n",
        "lgb = pd.read_csv(\"/content/previsioni_migliori_LGB.csv\")\n",
        "ada = pd.read_csv(\"/content/previsioni_migliori_ADA.csv\")\n",
        "pa = pd.read_csv(\"/content/previsioni_migliori_PA.csv\")\n",
        "etc = pd.read_csv(\"/content/previsioni_migliori_ETC.csv\")\n",
        "\n",
        "\n",
        "lr.rename(columns={'label': 'LR'}, inplace=True)\n",
        "nb.rename(columns={'label': 'NB'}, inplace=True)\n",
        "xgb.rename(columns={'label': 'XGB'}, inplace=True)\n",
        "rf.rename(columns={'label': 'RF'}, inplace=True)\n",
        "svm.rename(columns={'label': 'SVM'}, inplace=True)\n",
        "mlp.rename(columns={'label': 'MLP'}, inplace=True)\n",
        "lgb.rename(columns={'label': 'LGB'}, inplace=True)\n",
        "ada.rename(columns={'label': 'ADA'}, inplace=True)\n",
        "pa.rename(columns={'label': 'PA'}, inplace=True)\n",
        "etc.rename(columns={'label': 'ETC'}, inplace=True)\n",
        "\n",
        "final_df = pd.concat([lr, nb, xgb, rf, svm, mlp, lgb, ada, pa, etc], axis=1)\n",
        "final_df.to_csv('merged_labels.csv', index=False)"
      ],
      "metadata": {
        "id": "HAmcpKKOSiHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### make the final prediction"
      ],
      "metadata": {
        "id": "qzbJMizmnAdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = final_df.mode(axis=1).iloc[:, 0].astype(int)\n",
        "output_df = pd.DataFrame({'label': labels})\n",
        "\n",
        "output_df.to_csv('previsioni_finali_lr_terminato.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'test_predictions.csv, daje'\")"
      ],
      "metadata": {
        "id": "mOSeYci8S2M7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}